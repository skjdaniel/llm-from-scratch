{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Llama 3.1 to Llama 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct Llama 3.2 1B.\n",
    "\n",
    "I don't train the model or load model weights from elsewhere.\n",
    "\n",
    "Raschka defines a `SharedBuffer` class so that we can reuse the `mask`, `sin`, and `cos` tensors in the transformer blocks. I don't implement this here.\n",
    "\n",
    "Part of the code for the RoPE implementation is copied from Raschka's repo (the section between `#New section` and `#End new section` ).\n",
    "\n",
    "Differences between Llama 3.1 8B and Llama 3.2 1B:\n",
    "- Llama 3.2 uses weight tying (the weights of the embedding layer are used for the output layer).\n",
    "- Llama 3.2 has the same `context_length` as Llama 3.1 (131,072), but has half the embedding dimension (2,048 rather than 4,096) and half the number of transformer blocks (16 rather than 32). The dimension of the hidden layer in the transformer MLP is also much less (8,192 compared with Llama 3.1's 14,336).\n",
    "- One of the RoPE parameters is different.\n",
    "\n",
    "In this notebook:\n",
    "- Imports.\n",
    "- Llama 3.2 RoPE parameters.\n",
    "- RoPE implementation.\n",
    "- Grouped-query attention.\n",
    "- Transformer block.\n",
    "- Llama 3.2 model class.\n",
    "- Llama 3.2 1B config.\n",
    "- Instantiate toy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2 RoPE parameters\n",
    "\n",
    "- Same as Llama 3.1, except for `freq_config['factor']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.1\n",
    "# theta_base = 500_000\n",
    "# context_length= 131_072\n",
    "# freq_config = {\n",
    "#     \"factor\": 8.0,\n",
    "#     \"low_freq_factor\": 1.0,\n",
    "#     \"high_freq_factor\": 4.0,\n",
    "#     \"original_context_length\": 8192\n",
    "# }\n",
    "\n",
    "# Llama 3.2\n",
    "theta_base = 500_000\n",
    "context_length= 131_072\n",
    "freq_config = {\n",
    "    \"factor\": 32.0,\n",
    "    \"low_freq_factor\": 1.0,\n",
    "    \"high_freq_factor\": 4.0,\n",
    "    \"original_context_length\": 8192\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement RoPE\n",
    "\n",
    "- Same structure as Llama 3.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(d, theta_base=theta_base, \n",
    "                        context_length=context_length,\n",
    "                        freq_config=freq_config):\n",
    "    div_term = torch.exp(torch.arange(0, d, 2)[: (d // 2)].float() * (-torch.log(torch.tensor(theta_base)) / d))\n",
    "    inv_freq = div_term\n",
    "    \n",
    "    # New section\n",
    "    low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "    high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "\n",
    "    wavelen = 2 * torch.pi / inv_freq\n",
    "    inv_freq_llama = torch.where(\n",
    "        wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n",
    "    )\n",
    "\n",
    "    smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n",
    "        freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "    )\n",
    "\n",
    "    smoothed_inv_freq = (\n",
    "        (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n",
    "    )\n",
    "\n",
    "    is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n",
    "    inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "    # End new section\n",
    "    \n",
    "    inv_freq = div_term\n",
    "    positions = rearrange(torch.arange(0, context_length, dtype=torch.float), 'i -> i 1')\n",
    "    angles = positions * div_term\n",
    "    angles = torch.cat([angles, angles], dim=-1)\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "    return cos, sin\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    b, h, t, d = x.shape\n",
    "    assert d % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    x1 = x[:, :, :, : d // 2]\n",
    "    x2 = x[:, :, :, d // 2 :]\n",
    "\n",
    "    cos = rearrange(cos[: t, :], 't d -> 1 1 t d')\n",
    "    sin = rearrange(sin[: t, :], 't d -> 1 1 t d')\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = x * cos + rotated * sin\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped-query attention\n",
    "\n",
    "- Same as Llama 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_k, d_v, \n",
    "                    context_length, n_heads,\n",
    "                    n_kv_groups, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert n_heads % n_kv_groups == 0, \"Number of heads must be divisible by number of key-value groups\"\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_groups = n_kv_groups\n",
    "        self.group_size = n_heads // n_kv_groups\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.wq = nn.Linear(d_model, n_heads * d_k, bias=False, dtype=dtype)\n",
    "        self.wk = nn.Linear(d_model, n_kv_groups * d_k, bias=False, dtype=dtype)\n",
    "        self.wv = nn.Linear(d_model, n_kv_groups * d_v, bias=False, dtype=dtype)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_model, bias=False, dtype=dtype)     \n",
    "        \n",
    "        self.register_buffer('mask', \n",
    "            torch.triu(torch.ones(context_length, context_length), \n",
    "            diagonal=1))   \n",
    "            \n",
    "        cos, sin = precompute_rope_params(d=self.d_k, context_length=context_length)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin) \n",
    "\n",
    "    def forward(self, x):\n",
    "        q = rearrange(self.wq(x), 'b t (h k) -> b h t k', h=self.n_heads)\n",
    "        k = rearrange(self.wk(x), 'b t (nkv k) -> b nkv t k', nkv=self.n_kv_groups)\n",
    "        v = rearrange(self.wv(x), 'b t (nkv v) -> b nkv t v', nkv=self.n_kv_groups)\n",
    "\n",
    "        q = compute_rope(q, self.cos, self.sin)\n",
    "        k = compute_rope(k, self.cos, self.sin)\n",
    "\n",
    "        k = repeat(k, 'b nkv t k -> b (nkv gsz) t k', gsz=self.group_size)\n",
    "        v = repeat(v, 'b nkv t v -> b (nkv gsz) t v', gsz=self.group_size)\n",
    "        \n",
    "        attn = torch.einsum('bhtk, bhsk -> bhts', q, k) / self.d_k**0.5\n",
    "        mask_bool = self.mask.bool()[:x.size(1), :x.size(1)]\n",
    "        attn = attn.masked_fill(mask_bool, -torch.inf)\n",
    "        attn = F.softmax(attn, dim=3)\n",
    "        out = torch.einsum('bhts, bhsv -> bhtv', attn, v)\n",
    "        out = rearrange(out, 'b h t v -> b t (h v)')\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer block\n",
    "\n",
    "- Same as Llama 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = GroupedQueryAttention(\n",
    "            cfg['d_model'], cfg['d_k'], cfg['d_v'], \n",
    "            cfg['context_length'], cfg['n_heads'],\n",
    "            cfg['n_kv_groups'], cfg['dtype'])\n",
    "        self.norm1 = nn.RMSNorm(cfg['d_model'])\n",
    "        self.fc1 = nn.Linear(cfg['d_model'], cfg['hidden_dim'],\n",
    "                        dtype=cfg['dtype'], bias=False) \n",
    "        self.fc2 = nn.Linear(cfg['d_model'], cfg['hidden_dim'],\n",
    "                        dtype=cfg['dtype'], bias=False) \n",
    "        self.fc3 = nn.Linear(cfg['hidden_dim'], cfg['d_model'],\n",
    "                        dtype=cfg['dtype'], bias=False)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.norm2 = nn.RMSNorm(cfg['d_model'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.attn(self.norm1(x))\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.silu(self.fc1(x)) * self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2 model class\n",
    "\n",
    "- Same as Llama 3.1 except for the weight tying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3_2Model(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            cfg['vocab_size'], \n",
    "            cfg['d_model'], \n",
    "            dtype=cfg['dtype']\n",
    "            )\n",
    "        self.trf_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(cfg) for _ in range(cfg['n_blocks'])\n",
    "        ])\n",
    "        self.final_norm = nn.RMSNorm(cfg['d_model'])    \n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['d_model'], \n",
    "            cfg['vocab_size'],\n",
    "            bias=False,\n",
    "            dtype=cfg['dtype']\n",
    "            )\n",
    "        self.out_head.weight = nn.Parameter(self.token_embedding.weight.T)\n",
    "        self.out_head.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.token_embedding(x)\n",
    "        x = self.trf_blocks(x)  \n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2 1B config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA3_1_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,     \n",
    "    \"context_length\": 131_072, \n",
    "    \"d_model\": 4096,\n",
    "    \"d_k\": 128,\n",
    "    \"d_v\": 128,       \n",
    "    \"n_heads\": 32,  \n",
    "    \"n_kv_groups\": 8,         \n",
    "    \"n_blocks\": 32,          \n",
    "    \"hidden_dim\": 14_336,    \n",
    "    \"dtype\": torch.bfloat16  \n",
    "}\n",
    "\n",
    "LLAMA3_2_CONFIG_1B = {\n",
    "    \"vocab_size\": 128_256,     \n",
    "    \"context_length\": 131_072, \n",
    "    \"d_model\": 2048,\n",
    "    \"d_k\": 128,\n",
    "    \"d_v\": 128,       \n",
    "    \"n_heads\": 32,  \n",
    "    \"n_kv_groups\": 8,         \n",
    "    \"n_blocks\": 16,          \n",
    "    \"hidden_dim\": 8192,    \n",
    "    \"dtype\": torch.bfloat16  \n",
    "}\n",
    "\n",
    "LLAMA3_2_CONFIG_TOY = {\n",
    "    \"vocab_size\": 128_256,     \n",
    "    \"context_length\": 1000, \n",
    "    \"d_model\": 64,\n",
    "    \"d_k\": 4,\n",
    "    \"d_v\": 4,       \n",
    "    \"n_heads\": 16,  \n",
    "    \"n_kv_groups\": 8,         \n",
    "    \"n_blocks\": 1,          \n",
    "    \"hidden_dim\": 64,    \n",
    "    \"dtype\": torch.bfloat16  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate toy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Llama3_2Model(\n",
       "  (token_embedding): Embedding(128256, 64)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wk): Linear(in_features=64, out_features=32, bias=False)\n",
       "        (wv): Linear(in_features=64, out_features=32, bias=False)\n",
       "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm((64,), eps=None, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (fc2): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (fc3): Linear(in_features=64, out_features=64, bias=False)\n",
       "      (silu): SiLU()\n",
       "      (norm2): RMSNorm((64,), eps=None, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm((64,), eps=None, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=64, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Llama3_2Model(LLAMA3_2_CONFIG_TOY)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,233,152 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
