{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GPT-2 (small) and load weights from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GPT-2 (small) from scratch but don't train the model. Instead, load weights from Hugging Face.\n",
    "\n",
    "- Imports, including `GPT2Model` from `transformers`.\n",
    "- `MultiHeadAttention` class.\n",
    "- Approximate GELU activation for use in transformer block.\n",
    "- Transformer block.\n",
    "- GPT model.\n",
    "- Model configuration. This is the same as in the llm-from-scratch notebook, except for the `context_length` (which is now 1024 instead of 256), and the bias for the queries, keys, and values (now set to `True`).\n",
    "- Instantiate the model.\n",
    "- Load GPT-2 (small) from Hugging Face.\n",
    "- Define functions to load the weights from the Hugging Face model into our model.\n",
    "- Load the weights into our model.\n",
    "- Define functions to generate text samples using top-k sampling.\n",
    "- Generate text samples with various values for k and for the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from transformers import GPT2Model\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_k, d_v, dropout,\n",
    "            context_length, n_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.wq = nn.Linear(d_model, n_heads * d_k, bias=qkv_bias)\n",
    "        self.wk = nn.Linear(d_model, n_heads * d_k, bias=qkv_bias)\n",
    "        self.wv = nn.Linear(d_model, n_heads * d_v, bias=qkv_bias)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_model) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', \n",
    "            torch.triu(torch.ones(context_length, context_length), \n",
    "            diagonal=1))    \n",
    "\n",
    "    def forward(self, x):\n",
    "        q = rearrange(self.wq(x), 'b t (h k) -> b h t k', h=self.n_heads)\n",
    "        k = rearrange(self.wk(x), 'b t (h k) -> b h t k', h=self.n_heads)\n",
    "        v = rearrange(self.wv(x), 'b t (h v) -> b h t v', h=self.n_heads)\n",
    "        attn = torch.einsum('bhtk, bhsk -> bhts', q, k) / self.d_k**0.5\n",
    "        mask_bool = self.mask.bool()[:x.size(1), :x.size(1)]\n",
    "        attn = attn.masked_fill(mask_bool, -torch.inf)\n",
    "        attn = F.softmax(attn, dim=3)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.einsum('bhts, bhsv -> bhtv', attn, v)\n",
    "        out = rearrange(out, 'b h t v -> b t (h v)')\n",
    "        return self.linear(out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproxGELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(0.79788456 * (x + 0.044715 * x**3)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            cfg['d_model'], cfg['d_k'], cfg['d_v'], \n",
    "            cfg['dropout'], cfg['context_length'], \n",
    "            cfg['n_heads'], cfg['qkv_bias'])\n",
    "        self.ln1 = nn.LayerNorm(cfg['d_model'])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cfg['d_model'], 4 * cfg['d_model']),\n",
    "            ApproxGELU(),\n",
    "            nn.Linear(4 * cfg['d_model'], cfg['d_model'])\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(cfg['d_model'])\n",
    "        self.dropout = nn.Dropout(cfg['dropout'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.dropout(self.attn(self.ln1(x)))\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.dropout(self.mlp(self.ln2(x)))\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTVerdict(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['d_model'])\n",
    "        self.position_embedding = nn.Embedding(cfg['context_length'], cfg['d_model'])\n",
    "        self.dropout = nn.Dropout(cfg['dropout'])\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg) for _ in range(cfg['n_blocks'])\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(cfg['d_model'])\n",
    "        self.out_head = nn.Linear(cfg['d_model'], cfg['vocab_size'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.token_embedding(x)\n",
    "        x = x + self.position_embedding(torch.arange(t, device=x.device))   \n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'd_model': 768,\n",
    "    'd_k': 64,\n",
    "    'd_v': 64,\n",
    "    'n_heads': 12,\n",
    "    'n_blocks': 12,\n",
    "    'dropout': 0.1,\n",
    "    'qkv_bias': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTVerdict(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GPT-2 (small) from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_hf = GPT2Model.from_pretrained('gpt2', cache_dir='models')\n",
    "gpt_hf.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_check(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(right.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(gpt, gpt_hf):\n",
    "\n",
    "    d = gpt_hf.state_dict()\n",
    "\n",
    "    gpt.position_embedding.weight = assign_check(gpt.position_embedding.weight, d[\"wpe.weight\"])\n",
    "    gpt.token_embedding.weight = assign_check(gpt.token_embedding.weight, d[\"wte.weight\"])\n",
    "    \n",
    "    for b in range(BASE_CONFIG[\"n_blocks\"]):\n",
    "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
    "        gpt.blocks[b].attn.wq.weight = assign_check(gpt.blocks[b].attn.wq.weight, q_w.T)\n",
    "        gpt.blocks[b].attn.wk.weight = assign_check(gpt.blocks[b].attn.wk.weight, k_w.T)\n",
    "        gpt.blocks[b].attn.wv.weight = assign_check(gpt.blocks[b].attn.wv.weight, v_w.T)\n",
    "    \n",
    "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
    "        gpt.blocks[b].attn.wq.bias = assign_check(gpt.blocks[b].attn.wq.bias, q_b)\n",
    "        gpt.blocks[b].attn.wk.bias = assign_check(gpt.blocks[b].attn.wk.bias, k_b)\n",
    "        gpt.blocks[b].attn.wv.bias = assign_check(gpt.blocks[b].attn.wv.bias, v_b)\n",
    "    \n",
    "    \n",
    "        gpt.blocks[b].attn.linear.weight = assign_check(gpt.blocks[b].attn.linear.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
    "        gpt.blocks[b].attn.linear.bias = assign_check(gpt.blocks[b].attn.linear.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
    "    \n",
    "        gpt.blocks[b].mlp[0].weight = assign_check(gpt.blocks[b].mlp[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
    "        gpt.blocks[b].mlp[0].bias = assign_check(gpt.blocks[b].mlp[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
    "        gpt.blocks[b].mlp[2].weight = assign_check(gpt.blocks[b].mlp[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
    "        gpt.blocks[b].mlp[2].bias = assign_check(gpt.blocks[b].mlp[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
    "    \n",
    "        gpt.blocks[b].ln1.weight = assign_check(gpt.blocks[b].ln1.weight, d[f\"h.{b}.ln_1.weight\"])\n",
    "        gpt.blocks[b].ln1.bias = assign_check(gpt.blocks[b].ln1.bias, d[f\"h.{b}.ln_1.bias\"])\n",
    "        gpt.blocks[b].ln2.weight = assign_check(gpt.blocks[b].ln2.weight, d[f\"h.{b}.ln_2.weight\"])\n",
    "        gpt.blocks[b].ln2.bias = assign_check(gpt.blocks[b].ln2.bias, d[f\"h.{b}.ln_2.bias\"])\n",
    "    \n",
    "        gpt.ln.weight = assign_check(gpt.ln.weight, d[f\"ln_f.weight\"])\n",
    "        gpt.ln.bias = assign_check(gpt.ln.bias, d[f\"ln_f.bias\"])\n",
    "        gpt.out_head.weight = assign_check(gpt.out_head.weight, d[\"wte.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights(gpt, gpt_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_idxs(text, tokenizer):\n",
    "    idxs = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    return rearrange(torch.tensor(idxs), 'n -> 1 n')\n",
    "\n",
    "def idxs_to_text(idxs, tokenizer):\n",
    "    idxs = rearrange(idxs, '1 n -> n')\n",
    "    return tokenizer.decode(idxs.tolist())\n",
    "    \n",
    "def generate_topk(model, start_context, context_size, max_new_tokens, \n",
    "                    top_k=None, temperature=0.0):\n",
    "    idx_batch = text_to_idxs(start_context, tokenizer).to(device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        cropped_idx_batch = idx_batch[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(cropped_idx_batch)  \n",
    "        next_token_logits = logits[:, -1, :]  # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_k_logits, _ = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "            min_val = top_k_logits[:, -1]\n",
    "            next_token_logits = torch.where(next_token_logits < min_val, \n",
    "                                            torch.ones_like(next_token_logits) * -float('inf'), \n",
    "                                            next_token_logits)\n",
    "    \n",
    "        if temperature > 0.0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(next_token_probs, num_samples=1)\n",
    "\n",
    "\n",
    "        else:\n",
    "            next_token_idx = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        if next_token_idx.item() == BASE_CONFIG['vocab_size'] - 1:\n",
    "            break\n",
    "        \n",
    "        idx_batch = torch.cat((idx_batch, next_token_idx), dim=1)\n",
    "    \n",
    "    print(idxs_to_text(idx_batch, tokenizer).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k: 1, temperature: 0.0\n",
      "Every effort moves you can make to get the most out of your work.  The best way to do this is to make a list of your work.  \n",
      "top_k: 1, temperature: 0.5\n",
      "Every effort moves you to the right.  The right is the one that is the one that is the one that is the one that is the one that is the\n",
      "top_k: 1, temperature: 1.0\n",
      "Every effort moves you to the next level.  The next level is the most important.  The next level is the most important.  The next level\n",
      "top_k: 1, temperature: 1.5\n",
      "Every effort moves you to the next step.  The first step is to make sure you are doing the right thing.  The second step is to make sure\n",
      "top_k: 1, temperature: 2.0\n",
      "Every effort moves you forward.  The first thing you need to do is to understand the way that you are doing things. You are not doing things. \n",
      "top_k: 3, temperature: 0.0\n",
      "Every effort moves you to get the most out of your time.  The best way to get the most out of your time is to get the most out of your\n",
      "top_k: 3, temperature: 0.5\n",
      "Every effort moves you forward.  It's not just a matter of getting your feet wet. It's a matter of getting your feet wet.  The first\n",
      "top_k: 3, temperature: 1.0\n",
      "Every effort moves you to the place you into the world.  The world is a place where you can be. You can't just be there to be there.\n",
      "top_k: 3, temperature: 1.5\n",
      "Every effort moves you forward, it moves you to the next. You can't just move on a straight path, you have to move in the same way. It's\n",
      "top_k: 3, temperature: 2.0\n",
      "Every effort moves you. It is a process of action and a process of action.  It's a process.  It's a process, a process.\n",
      "top_k: 5, temperature: 0.0\n",
      "Every effort moves you to the right.   The more you do, the more you will be able to do it.  The more you do, the\n",
      "top_k: 5, temperature: 0.5\n",
      "Every effort moves you through the game and the game.  The game is about the game and the game is about the game.  The game is about the\n",
      "top_k: 5, temperature: 1.0\n",
      "Every effort moves you to the next step.  If you are not a big fan of a lot of things in life and you are a fan of the whole \"\n",
      "top_k: 5, temperature: 1.5\n",
      "Every effort moves you from being the most powerful, and it is not enough. Thesaying: The most important thing you need to do is make sure that what you\n",
      "top_k: 5, temperature: 2.0\n",
      "Every effort moves you through your life, but the way out is the one you can do with a lot of effort and a good amount of work and it's going to\n",
      "top_k: 10, temperature: 0.0\n",
      "Every effort moves you to the end of the day.  The only way to get to the end of the day is to get to the end of the day.\n",
      "top_k: 10, temperature: 0.5\n",
      "Every effort moves you to get the right item.  If you're going to use the right item, you have to use the right item.  If you\n",
      "top_k: 10, temperature: 1.0\n",
      "Every effort moves you forward with the whole thing, but at the same time it's also a lot more difficult to get to the point where you can make some sense of\n",
      "top_k: 10, temperature: 1.5\n",
      "Every effort moves you from this to become your own and become something you can be. I am in my body, but I do not know what is.  It\n",
      "top_k: 10, temperature: 2.0\n",
      "Every effort moves you to create the best results.   You may have found the latest and best.\n",
      "top_k: 100, temperature: 0.0\n",
      "Every effort moves you to the next step.  The next step is to get to the next step.  The next step is to get to the next step\n",
      "top_k: 100, temperature: 0.5\n",
      "Every effort moves you. It's all you. Just like the past.  That's all it is.  You are, after all.  I\n",
      "top_k: 100, temperature: 1.0\n",
      "Every effort moves you in my way – there was one before me!\"  Although the person was considered to be the toughest because they had no understanding of the idea,\n",
      "top_k: 100, temperature: 1.5\n",
      "Every effort moves you over on stage on a huge rock to catch the moment on The Day's Out; then go to hit target – and double, with this move you\n",
      "top_k: 100, temperature: 2.0\n",
      "Every effort moves you on. . . . One by chance from the place I, by a man whose mind comes what does a little, not be but as was right\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for top_k in [1, 3, 5, 10, 100]:\n",
    "    for temperature in [0.0, 0.5, 1.0, 1.5, 2.0]:\n",
    "        print(f'top_k: {top_k}, temperature: {temperature}')\n",
    "        generate_topk(model=gpt,\n",
    "                start_context=\"Every effort moves you\",\n",
    "                context_size=BASE_CONFIG[\"context_length\"],\n",
    "                max_new_tokens=30,\n",
    "                top_k=top_k,\n",
    "                temperature=temperature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
